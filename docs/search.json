[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Docker Workshop üê≥",
    "section": "",
    "text": "Welcome to the Docker for Data Science workshop! As its name implies, this workshop focuses on the foundations of Docker with data science applications. That includes the following topics:",
    "crumbs": [
      "Home",
      "Docker Workshop üê≥"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "tests/test3.html",
    "href": "tests/test3.html",
    "title": "Test Quarto",
    "section": "",
    "text": "Loading libraries:\n\n\nCode\nimport pandas as pd\n\n\nUse the print command:\n\n\nCode\nprint(\"Hello World!\")\n\n\nHello World!"
  },
  {
    "objectID": "tests/test2.html",
    "href": "tests/test2.html",
    "title": "sdsu-docker-workshop",
    "section": "",
    "text": "import pandas as pd\n\n\nprint(\"Hello World!\")\n\nHello World!"
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "Docker Workshop üê≥",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThe main learning object is to motivate data scientists and other partitioners to use containers in their workflow. Docker has some learning curve, and the goal of this workshop is to reduce the entry barrier for new learners. No prior knowledge of Docker is needed, but it is recommended to have some familiarity with basic command line tools.\nBy the end of this tutorial, you will:\n\nUnderstand the general applications of Docker, and in particular, the ones for data science\nLearn about the advantages and disadvantages of using Docker\nAble to design, build, and run a container",
    "crumbs": [
      "Home",
      "Docker Workshop üê≥"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Docker Workshop üê≥",
    "section": "Schedule",
    "text": "Schedule\nThe workshop will take place on Feb 5: 1:00 - 5:00 pm @ Student Union:\n\n1:00 - 1:30 Introduction to Docker\n1:30 - 2:00 General settings\n2:00 - 2:15 Workflow\n2:15 - 2:30 Break\n2:15 - 2:45 The Dockerfile\n2:45 - 3:15 Build\n3:15 - 3:45 Run\n3:45 - 4:15 Docker Compose\n4:15 - 5:00 General topics",
    "crumbs": [
      "Home",
      "Docker Workshop üê≥"
    ]
  },
  {
    "objectID": "index.html#general-requirements",
    "href": "index.html#general-requirements",
    "title": "Docker Workshop üê≥",
    "section": "General Requirements",
    "text": "General Requirements\nTo best utilize the workshop time, please install Docker Desktop prior to the workshop and set up a Docker Hub account. Docker Desktop is free for nonenterprise use cases.\nI am going to use VScode throughout the demos, and I recommend installing VScode as well to follow along.\nThe Settings section provides the installation instructions.",
    "crumbs": [
      "Home",
      "Docker Workshop üê≥"
    ]
  },
  {
    "objectID": "01-settings.html",
    "href": "01-settings.html",
    "title": "Settings",
    "section": "",
    "text": "To follow this tutorial, you‚Äôll need to set up Docker Desktop and VScode. Plus, you‚Äôll have to create an account on Docker Hub. This section is dedicated to the workshop settings and covers the installation and configuration of Docker and VScode.",
    "crumbs": [
      "Home",
      "Settings"
    ]
  },
  {
    "objectID": "01-settings.html#setting-docker",
    "href": "01-settings.html#setting-docker",
    "title": "Settings",
    "section": "Setting Docker",
    "text": "Setting Docker\nVarious ways exist to build and run Docker images on different operations systems. For the purpose of this guide, we will be utilizing Docker Desktop. It is a user-friendly container management interface that is compatible with MacOS, Windows, and Linux operating systems.\nNote: Docker Desktop is free for personal use but requires a license for commercial use. For further information, please refer to https://www.docker.com/pricing/.\nTo install Docker Desktop, go to Docker website and follow the installation instructions according to your OS:\n\n\n\nFigure 1 - Docker Desktop download page",
    "crumbs": [
      "Home",
      "Settings"
    ]
  },
  {
    "objectID": "01-settings.html#docker-hub",
    "href": "01-settings.html#docker-hub",
    "title": "Settings",
    "section": "Docker Hub",
    "text": "Docker Hub\nContainer Registry has a similar functionality as Github for code, and it uses to store and share images. There are many container registries, and the most common is Docker Hub. We will use throughout the tutorial Docker Hub to pull different images, such as Python built-in images. To register and create an account go to https://hub.docker.com and follow the registration instructions.\nAfter installing Docker Desktop and setting account on Docker Hub, open Docker Desktop, and from the command line, login to Docker Hub:\n docker login\nYou will have to enter your username and password, and you should expect the following output if the login is successful:\nLogin with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.\nUsername: rkrispin\nPassword:\nLogin Succeeded\n\nLogging in with your password grants your terminal complete access to your account.\nFor better security, log in with a limited-privilege personal access token. Learn more at https://docs.docker.com/go/access-tokens/\nNote: Docker Hub is completely public (for the free tier). Any image you push and store there will be available for all other users. Regardless if your container registry is public or not, NEVER store credentials, passwords, or any other sensitive information on your Docker images.\n\nHello World!\nThere is no better way to test if Docker was installed properly than by running whalesay (or üê≥ say) Docker‚Äôs most Hello World! common example. The whalesay is an adaption of the Linux cowsay (üêÆ say) game using a whale instead of a cow to print some text. Let‚Äôs run the below code from the terminal to print Welcome to the Docker for Data Science Workshop! üëãüèº:\ndocker run docker/whalesay cowsay Welcome to the Docker for Data Science Workshop! üëãüèº\nIf this is the first time you are using Docker or your first time using the whalesay image you should expect the following message:\nUnable to find image 'docker/whalesay:latest' locally\nThat is a generic message that notifies that the requested image cannot be found locally, and Docker will try to pull the image from the hub (if specified) and follow by downloading the image:\nlatest: Pulling from docker/whalesay\nImage docker.io/docker/whalesay:latest uses outdated schema1 manifest format. Please upgrade to a schema2 image for better future compatibility. More information at https://docs.docker.com/registry/spec/deprecated-schema-v1/\ne190868d63f8: Pull complete\n909cd34c6fd7: Pull complete\n0b9bfabab7c1: Pull complete\na3ed95caeb02: Pull complete\n00bf65475aba: Pull complete\nc57b6bcc83e3: Pull complete\n8978f6879e2f: Pull complete\n8eed3712d2cf: Pull complete\nDigest: sha256:178598e51a26abbc958b8a2e48825c90bc22e641de3d31e18aaf55f3258ba93b\nStatus: Downloaded newer image for docker/whalesay:latest\nAnd this is the expected output:\n\n/ Welcome to the Docker for Data Science \\\n\\ Workshop! üëãüèº                           /\n ----------------------------------------\n    \\\n     \\\n      \\\n                    ##        .\n              ## ## ##       ==\n           ## ## ## ##      ===\n       /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ ===\n  ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ /  ===- ~~~\n       \\______ o          __/\n        \\    \\        __/\n          \\____\\______/ \nIf you are able to run the whalesay app you are ready to get started with Docker.",
    "crumbs": [
      "Home",
      "Settings"
    ]
  },
  {
    "objectID": "01-settings.html#installing-vscode",
    "href": "01-settings.html#installing-vscode",
    "title": "Settings",
    "section": "Installing VScode",
    "text": "Installing VScode\nInstalling VScode is straightforward - go to the VScode website https://code.visualstudio.com/ and click on the Download button (purple rectangle on the screenshot):\n\n\n\nFigure 2 - Visual Studio Code download page\n\n\nDownload the installation file and follow the instructions.\nTo set the Python environment with Docker in VScode we will need the following extensions:\n\nDev Containers - this extension enables to open a folder and execute a code inside a Docker container (more info available here)\nPython - the main Python plug-in for VScode, enables to execute, debugging, code navigation, code formatting, etc. (more info available here)\n\nHere is how to install an extension on VScode: - Click the Extensions button on the left menu (mark with a purple arrow on the screenshot below) - Type the extension name on the search menu (see the yellow rectangular). You can see the search results below, and clicking on each extension will open a window with the extension details. - Last but not least, Click the install button (see the green rectangular) to install the extension\n\n\n\nFigure 3 - Steps to install extension on VScode\n\n\n\nNote: The Dev Containers extension is required to launch the dockerized environment. We will see later in this tutorial how to set and install the necessary extensions for your dockerized environment automatically with the devcontainer.json file.",
    "crumbs": [
      "Home",
      "Settings"
    ]
  },
  {
    "objectID": "03-the_dockerfile.html",
    "href": "03-the_dockerfile.html",
    "title": "The Dockerfile",
    "section": "",
    "text": "The Dockerfile provides a set of instructions for the Docker engine about how to build the image. You can think about it as the image‚Äôs recipe. It has its own unique and intuitive syntax using the following structure:\nCOMMAND some instructions\nFor example, the following Dockerfile imports the official Python (version 3.10) image as the base image and then using the apt-get update and apt-get install to install the curl library :\n./examples/ex-1/Dockerfile\nFROM python:3.10\n\nLABEL example=1\n\nENV PYTHON_VER=3.10\n\nRUN apt-get update && apt-get install -y --no-install-recommends curl\nIn a nutshell, we used the FROM command to specify the image we want to import from the Docker registry (don‚Äôt forget to login to the Docker registry service you are using before building the image!). The LABEL command is used to set labels or comments, and the ENV command is to set environment variables. Last but not least, the RUN command is used to run a command on the command line, in this case, to install the curl library.\nLet‚Äôs now review the Dockerfile core commands:\n\nFROM - Defines the base image to use for the image‚Äôs build. In most cases, unless you are building the image from scratch, you will use some base image with some pre-installed OS and some dependencies. For example, in this tutorial, we will import as our base image the official Python image\nLABEL - Enables to add information about the image to the image‚Äôs metadata, such as authors, maintainers, license, etc.\nENV - Uses to set environment variables\nARG - Enables to set parameters during the build time\nRUN - Allows executing CLI commands (e.g., pip install ..., apt-get ..., apt-install..., wget..., etc.) during the build time to add additional components to the base image\nCOPY - Enables to copy objects (e.g., files and folders) from your local system to the image\nWORKDIR - Sets the working directory inside the image\nEXPOSE - Defines the port number to expose the image during the run time\nCMD - Sets a default command to execute during the run time of the image\nENDPOINT - Allows configuring a container that will run as an executable\n\nDo not worry if, at this point, you do not fully understand the use cases of some of those commands. It will make more sense when we start to build images in the next section.",
    "crumbs": [
      "Home",
      "The Dockerfile"
    ]
  },
  {
    "objectID": "04-build.html",
    "href": "04-build.html",
    "title": "Docker Build",
    "section": "",
    "text": "Once the Dockerfile is ready, the next step is to build the image using the docker build command from the command line. For example, let‚Äôs build the above Dockerfile using the build command from this repo root folder:\ndocker build . -f ./examples/ex-1/Dockerfile -t rkrispin/vscode-python:ex1 \nHere are the arguments we used with the build command:\n\nThe -f tag defines the Dockerfile path. This argument is optional and should be used if you are calling the build function from a different folder than one of the Dockerfile\nThe . symbol defines the context folder of the files system as the one of the Dockerfile. Although we did not use the file system in this case, this enables us in other cases to call and copy files from our local folder to the image during the build time\nThe -t is used to set the image‚Äôs name and tag (e.g., version). In this case, the image name is rkrispin/vscode-python and the tag is ex1.\n\nYou should expect the following output:\n[+] Building 94.2s (6/6) FINISHED                                                                                                                                                                                                  \n =&gt; [internal] load build definition from Dockerfile                                                                                                                                                                          0.0s\n =&gt; =&gt; transferring dockerfile: 162B                                                                                                                                                                                          0.0s\n =&gt; [internal] load .dockerignore                                                                                                                                                                                             0.0s\n =&gt; =&gt; transferring context: 2B                                                                                                                                                                                               0.0s\n =&gt; [internal] load metadata for docker.io/library/python:3.10                                                                                                                                                                6.0s\n =&gt; [1/2] FROM docker.io/library/python:3.10@sha256:a8462db480ec3a74499a297b1f8e074944283407b7a417f22f20d8e2e1619782                                                                                                         82.1s\n =&gt; =&gt; resolve docker.io/library/python:3.10@sha256:a8462db480ec3a74499a297b1f8e074944283407b7a417f22f20d8e2e1619782                                                                                                          0.0s\n =&gt; =&gt; sha256:a8462db480ec3a74499a297b1f8e074944283407b7a417f22f20d8e2e1619782 1.65kB / 1.65kB                                                                                                                                0.0s\n =&gt; =&gt; sha256:4a1aacea636cab6af8f99f037d1e56a4de97de6025da8eff90b3315591ae3617 2.01kB / 2.01kB                                                                                                                                0.0s\n =&gt; =&gt; sha256:23e11cf6844c334b2970fd265fb09cfe88ec250e1e80db7db973d69d757bdac4 7.53kB / 7.53kB                                                                                                                                0.0s\n =&gt; =&gt; sha256:bba7bb10d5baebcaad1d68ab3cbfd37390c646b2a688529b1d118a47991116f4 49.55MB / 49.55MB                                                                                                                             26.1s\n =&gt; =&gt; sha256:ec2b820b8e87758dde67c29b25d4cbf88377601a4355cc5d556a9beebc80da00 24.03MB / 24.03MB                                                                                                                             11.0s\n =&gt; =&gt; sha256:284f2345db055020282f6e80a646f1111fb2d5dfc6f7ee871f89bc50919a51bf 64.11MB / 64.11MB                                                                                                                             26.4s\n =&gt; =&gt; sha256:fea23129f080a6e28ebff8124f9dc585b412b1a358bba566802e5441d2667639 211.00MB / 211.00MB                                                                                                                           74.5s\n =&gt; =&gt; sha256:7c62c924b8a6474ab5462996f6663e07a515fab7f3fcdd605cae690a64aa01c7 6.39MB / 6.39MB                                                                                                                               28.2s\n =&gt; =&gt; extracting sha256:bba7bb10d5baebcaad1d68ab3cbfd37390c646b2a688529b1d118a47991116f4                                                                                                                                     1.6s\n =&gt; =&gt; sha256:c48db0ed1df2d2df2dccd680323097bafb5decd0b8a08f02684b1a81b339f39b 17.15MB / 17.15MB                                                                                                                             31.9s\n =&gt; =&gt; extracting sha256:ec2b820b8e87758dde67c29b25d4cbf88377601a4355cc5d556a9beebc80da00                                                                                                                                     0.6s\n =&gt; =&gt; sha256:f614a567a40341ac461c855d309737ebccf10a342d9643e94a2cf0e5ff29b6cd 243B / 243B                                                                                                                                   28.4s\n =&gt; =&gt; sha256:00c5a00c6bc24a1c23f2127a05cfddd90865628124100404f9bf56d68caf17f4 3.08MB / 3.08MB                                                                                                                               29.4s\n =&gt; =&gt; extracting sha256:284f2345db055020282f6e80a646f1111fb2d5dfc6f7ee871f89bc50919a51bf                                                                                                                                     2.5s\n =&gt; =&gt; extracting sha256:fea23129f080a6e28ebff8124f9dc585b412b1a358bba566802e5441d2667639                                                                                                                                     6.2s\n =&gt; =&gt; extracting sha256:7c62c924b8a6474ab5462996f6663e07a515fab7f3fcdd605cae690a64aa01c7                                                                                                                                     0.3s\n =&gt; =&gt; extracting sha256:c48db0ed1df2d2df2dccd680323097bafb5decd0b8a08f02684b1a81b339f39b                                                                                                                                     0.5s\n =&gt; =&gt; extracting sha256:f614a567a40341ac461c855d309737ebccf10a342d9643e94a2cf0e5ff29b6cd                                                                                                                                     0.0s\n =&gt; =&gt; extracting sha256:00c5a00c6bc24a1c23f2127a05cfddd90865628124100404f9bf56d68caf17f4                                                                                                                                     0.2s\n =&gt; [2/2] RUN apt-get update && apt-get install -y --no-install-recommends curl                                                                                                                                               5.9s\n =&gt; exporting to image                                                                                                                                                                                                        0.1s\n =&gt; =&gt; exporting layers                                                                                                                                                                                                       0.1s\n =&gt; =&gt; writing image sha256:a8e4c6d06c97e9a331a10128d1ea1fa83f3a525e67c7040c2410940312e946f5                                                                                                                                  0.0s\n =&gt; =&gt; naming to docker.io/rkrispin/vscode-python:ex1  \n\nNote: The above output of the build describes the different layers of the image. Don‚Äôt worry if, at this point, it looks and sounds like gibberish. Reading this output type will be easier after reading the next section, which focuses on the image layers.\nYou can use the docker images command to validate that the image was created successfully:\n&gt;docker images\nREPOSITORY                             TAG       IMAGE ID       CREATED        SIZE\nrkrispin/vscode-python                 ex1       a8e4c6d06c97   43 hours ago   1.02GB\nThe next section will focus on the image layers and caching process.\n\nThe image layers\nThe build process of Docker‚Äôs images is based on layers. Depending on the context, the docker engine takes each one of the Dockerfile commands during the build time and translates it either into layer or metadata. Dockerfile commands, such as FROM and RUN are translated into a layer, and commands, such as LABEL, ARG, ENV, and CMD are translated into metadata. For example, we can observe in the output of the build of rkrispin/vscode-python image above that there are two layers:\n\nThe first layer started with [1/2] FROM..., corresponding to the FROM python:3.10 line on the Dockerfile, which import the Python 3.10 official image\nThe second layer started with [2/2] RUN apt-get..., corresponding to the RUN command on the Dockerfile\n\n\n\n\nFigure 1 - Example of a build output with respect to the Dockerfile\n\n\n\nThe docker inspect command returns the image metadata details in a JSON format. That includes the envrioment variables, labels, layers and general metadata. In the following example, we will us jq to extract the layers information from the metadata JSON file:\n&gt; docker inspect rkrispin/vscode-python:ex1 | jq '.[] | .RootFS'\n{\n  \"Type\": \"layers\",\n  \"Layers\": [\n    \"sha256:332b199f36eb054386cd2931c0824c97c6603903ca252835cc296bacde2913e1\",\n    \"sha256:2f98f42985b15cbe098d2979fa9273e562e79177b652f1208ae39f97ff0424d3\",\n    \"sha256:964529c819bb33d3368962458c1603ca45b933487b03b4fb2754aa55cc467010\",\n    \"sha256:e67fb4bad8f42cca08769ee21bbe15aca61ab97d4a46b181e05fefe3a03ee06d\",\n    \"sha256:037f26f869124174b0d6b6d97b95a5f8bdff983131d5a1da6bc28ddbc73531a5\",\n    \"sha256:737cec5220379f795b727e6c164e36e8e79a51ac66a85b3e91c3f25394d99224\",\n    \"sha256:65f4e45c2715f03ed2547e1a5bdfac7baaa41883450d87d96f877fbe634f41a9\",\n    \"sha256:baef981f26963b264913e79bd0a1472bae389441022d71f559e9d186600d2629\",\n    \"sha256:88e1d36ff4812423afc93d5f6208f2783df314d5ecf6f961325c65e1dbf891da\"\n  ]\n}\n\nAs you can see from the image‚Äôs layers output above, the rkrispin/vscode-python:ex1 image has nine layers. Each layer is represented by its hash key (e.g., sha256:...), and it is cached on the backend. While we saw on the build output that the docker engine triggered two processes from the FROM and RUN commands, we ended up with nine layers as opposed to two. The main reason for that is related to the fact that when importing the baseline image, we inherited the imported image characteristics, including the layers. In this case, we used the FROM to import the official Python image, which included eight layers, and then added the 9th layer by executing the RUN commands. You can test it by pulling the baseline image and using the inspect command to review its layers:\n&gt; docker pull python:3.10\n3.10: Pulling from library/python\nbba7bb10d5ba: Already exists \nec2b820b8e87: Already exists \n284f2345db05: Already exists \nfea23129f080: Already exists \n7c62c924b8a6: Already exists \nc48db0ed1df2: Already exists \nf614a567a403: Already exists \n00c5a00c6bc2: Already exists \nDigest: sha256:a8462db480ec3a74499a297b1f8e074944283407b7a417f22f20d8e2e1619782\nStatus: Downloaded newer image for python:3.10\ndocker.io/library/python:3.10\n\n&gt; docker inspect python:3.10 | jq '.[] | .RootFS'\n{\n  \"Type\": \"layers\",\n  \"Layers\": [\n    \"sha256:332b199f36eb054386cd2931c0824c97c6603903ca252835cc296bacde2913e1\",\n    \"sha256:2f98f42985b15cbe098d2979fa9273e562e79177b652f1208ae39f97ff0424d3\",\n    \"sha256:964529c819bb33d3368962458c1603ca45b933487b03b4fb2754aa55cc467010\",\n    \"sha256:e67fb4bad8f42cca08769ee21bbe15aca61ab97d4a46b181e05fefe3a03ee06d\",\n    \"sha256:037f26f869124174b0d6b6d97b95a5f8bdff983131d5a1da6bc28ddbc73531a5\",\n    \"sha256:737cec5220379f795b727e6c164e36e8e79a51ac66a85b3e91c3f25394d99224\",\n    \"sha256:65f4e45c2715f03ed2547e1a5bdfac7baaa41883450d87d96f877fbe634f41a9\",\n    \"sha256:baef981f26963b264913e79bd0a1472bae389441022d71f559e9d186600d2629\"\n  ]\n}\n\n\nLayers caching\nOne of the cons of Docker is the image build time. As the level of complexity of the Dockerfile is higher (e.g., a large number of dependencies), the longer the build time. Sometimes, your build won‚Äôt execute as expected on the first try. Either some requirements are missing, or something breaks during the build time. This is where the use of caching helps in reducing the image rebuild time. Docker has smart mechanization that identifies if each layer should be built from scratch or can leverage a cached layer and save time. For example, let‚Äôs add to the previous example another command to install the vim editor. Generally, we can (and should) add it to the same apt-get we are using to install the curl package, but for the purpose of showing the layers caching functionality, we will run it separately:\n./examples/ex-2/Dockerfile\nFROM python:3.10\n\nLABEL example=1\n\nENV PYTHON_VER=3.10\n\nRUN apt-get update && apt-get install -y --no-install-recommends curl\n\nRUN apt-get update && apt-get install -y --no-install-recommends vim\nWe will use the below command to build this image and tag it as rkrispin/vscode-python:ex2:\ndocker build . -f ./examples/ex-2/Dockerfile -t rkrispin/vscode-python:ex2 --progress=plain\nYou should expect the following output (if ran the previous build):\n =&gt; [internal] load build definition from Dockerfile                                                                                                                                     0.0s\n =&gt; =&gt; transferring dockerfile: 234B                                                                                                                                                     0.0s\n =&gt; [internal] load .dockerignore                                                                                                                                                        0.0s\n =&gt; =&gt; transferring context: 2B                                                                                                                                                          0.0s\n =&gt; [internal] load metadata for docker.io/library/python:3.10                                                                                                                           0.0s\n =&gt; [1/3] FROM docker.io/library/python:3.10                                                                                                                                             0.0s\n =&gt; CACHED [2/3] RUN apt-get update && apt-get install -y --no-install-recommends curl                                                                                                   0.0s\n =&gt; [3/3] RUN apt-get update && apt-get install -y --no-install-recommends vim                                                                                                          34.3s\n =&gt; exporting to image                                                                                                                                                                   0.4s \n =&gt; =&gt; exporting layers                                                                                                                                                                  0.4s \n =&gt; =&gt; writing image sha256:be39eb0eb986f083a02974c2315258377321a683d8472bac15e8d5694008df35                                                                                             0.0s \n =&gt; =&gt; naming to docker.io/rkrispin/vscode-python:ex2   \nAs can be noticed from the above build output, the first and second layers already exist from the previous build. Therefore, the docker engine adds their cached layers to the image (as opposed to building them from scratch), and just builds the 3rd layer and installs the vim editor.\nNote: By default, the build output is concise and short. You can get more detailed output during the build time by adding the progress argument and setting it to plain:\n&gt; docker build . -f ./examples/ex-2/Dockerfile -t rkrispin/vscode-python:ex2 --progress=plain\n#1 [internal] load .dockerignore\n#1 transferring context: 2B done\n#1 DONE 0.0s\n\n#2 [internal] load build definition from Dockerfile\n#2 transferring dockerfile: 234B done\n#2 DONE 0.0s\n\n#3 [internal] load metadata for docker.io/library/python:3.10\n#3 DONE 0.0s\n\n#4 [1/3] FROM docker.io/library/python:3.10\n#4 DONE 0.0s\n\n#5 [2/3] RUN apt-get update && apt-get install -y --no-install-recommends curl\n#5 CACHED\n\n#6 [3/3] RUN apt-get update && apt-get install -y --no-install-recommends vim\n#6 CACHED\n\n#7 exporting to image\n#7 exporting layers done\n#7 writing image sha256:be39eb0eb986f083a02974c2315258377321a683d8472bac15e8d5694008df35 0.0s done\n#7 naming to docker.io/rkrispin/vscode-python:ex2 done\n#7 DONE 0.0s\nSince we already cached the 3rd layer on the previous build, all the layers in the above output are cached, and the run time is less than 1 second.\nWhen setting your Dockerfile, you should be minded and strategic to the layers caching process. The order of the layers does matter! The following images demonstrate when the docker engine will use cached layers and when to rebuild them. The first image illustrates the initial build:\n\n\n\nFigure 2 - Illustration of initial build of image. The left side represents the Dockerfile‚Äôs commands and the right one the coorisponding layers\n\n\n\nIn this case, we have a Dockerfile with four commands that are translated during the build time into four layers. What will happen if we add a fifth command and place it right after the third one? The docker engine will identify that the first and second commands in the Dockerfile did not change and, therefore, will use the corresponding cached layers (one and two), and rebuild the rest of the layers from scratch:\n\n\n\nFigure 3 - Illustration of the caching process during the rebuild of an image\n\n\n\nWhen planning your Dockerfile, if applicable, a good practice is to place the commands that will most likely stay the same and keep new updates to the end of the file if possible.\nThat was just the tip of the iceberg, and there is much more to learn about Docker. The next section will explore different methods to run Python inside a container.",
    "crumbs": [
      "Home",
      "Docker Build"
    ]
  },
  {
    "objectID": "05-run.html",
    "href": "05-run.html",
    "title": "Docker Run",
    "section": "",
    "text": "In the previous sections, we saw how to define the image requirements with the Dockerfile and build it with the build command. This section focuses on running Python inside a container using the docker run command.\n\nDocker run\nThe docker run or run command enables us to create and run a new container from an image. Typically, the run command is used to launch a dockerized application or server or to execute a code following the below syntax:\ndocker run [OPTIONS] IMAGE [COMMAND] [ARG...]\nFor example, we can use the run command with the official Python 3.10 image:\ndocker run python:3.10 \nSurprisingly (or not), nothing happened. To understand that better, we need to go back to the Dockerfile. Generally, images can be used to run: - Server - Application\nIn both cases, we use the Dockerfile to set and enable launching them during the run time. In the case of a server, we use on the Dockerfile the PORT and CMD commands to set the server‚Äôs port on the image and launch the server, respectively. We then use the run command and add the -p (or --publish list) option to map the server‚Äôs port with a local port. Similarly, to launch an application, we use the CMD command on the Dockerfile to define the launch command during the run time and use the --interactive and --tty options to launch the container in interactive mode, which enables us to access the application.\nLet‚Äôs now go back to the python:3.10 image and use the inspect command to check if the CMD command was defined:\n&gt; docker inspect python:3.10 | jq '.[] | .Config.Cmd'\n[\n  \"python3\"\n]\nNote: We used the jq library again to parse out from the JSON output the CMD metadata\nAs you can see, the CMD on the python:3.10 image is set to run the default Python launch command - python3, which launches Python during the run time. Let‚Äôs now add the --interactive and --tty options to run the container in an interactive mode:\n docker run --interactive --tty python:3.10 \nThis launches the default Python version on the image. We can then test it by using the print command to print Hello World!:\nPython 3.10.12 (main, Jun 14 2023, 18:40:54) [GCC 12.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; print(\"Hello World!\")\nHello World!\n&gt;&gt;&gt; \nOK, we have Python running inside a dockerized environment, so why should we not use it? Mainly due to the following reasons: - This is not a development environment, and it is harder (in my mind) to maintain and develop code from the terminal with respect to Python IDEs such as PyCharm or VScode. - By default, the docker run is an ephemeral process, and therefore, your code is lost when you shut down the container.\nWhile there are ways to overcome the above issues, it is still convoluted and not as efficient as using VScode. In the next section, we will see how to set and run Python code with VScode and the Dev Containers extension.",
    "crumbs": [
      "Home",
      "Docker Run"
    ]
  },
  {
    "objectID": "02-workflow.html",
    "href": "02-workflow.html",
    "title": "Workflow",
    "section": "",
    "text": "In my data science career, one major technical milestone (after learning how to code and adopt Git) was learning Docker and starting to use it. It opened a new world of opportunities, from automating processes to deploying data science work in production. This section defines what Docker is and the data science applications and use cases.",
    "crumbs": [
      "Home",
      "Workflow"
    ]
  },
  {
    "objectID": "02-workflow.html#what-is-docker",
    "href": "02-workflow.html#what-is-docker",
    "title": "Workflow",
    "section": "What is Docker?",
    "text": "What is Docker?\nDocker is a CI/CD tool that enables seamless code deployment from development to production environments. By creating OS-level virtualization, it can package an application and its dependencies in a virtual container and ship it between different environments. The main advantages of using Docker within your development environment are:\n\nReproducibility - Docker enables you to seamlessly package your code and its dependencies into a single container and execute, test, share, and deploy it with a high level of consistency\nCollaboration - Docker solves the dependencies madness when a team of developers works together on a specific project. Having a unified environment saves a ton of time during the development step. For example, if one developer gets some error, it is easy for other developers to reproduce the error and help debug it\nDeployment - Docker simplifies the code shipment from the development environment to the production",
    "crumbs": [
      "Home",
      "Workflow"
    ]
  },
  {
    "objectID": "02-workflow.html#docker-for-data-science",
    "href": "02-workflow.html#docker-for-data-science",
    "title": "Workflow",
    "section": "Docker for Data Science",
    "text": "Docker for Data Science\nDocker was built to solve a common DevOps problem - the lack of reproducibility when shifting code between different environments (e.g., dev to prod). Reproducibility is not limited to DevOps, and it plays a critical role in the field of data science. We can define reproducibility as the ability to generate the exact outcome when running the same code regardless of the user or machine on which the code is running.\nThe first time I heard the term reproducibility was during my bachelor‚Äôs degree, where I learned that reproducibility starts and ends by setting a seed number to lock down random numbers. My favorite seed number is 12345. When I started to work as a data scientist, I realized that reproducibility goes beyond setting a seed number. Here are the main elements that can impact code reproducibility:\n\nVersion control ‚Äî First and foremost, reproducing the same results starts with the ability to track changes in your code Randomization ‚Äî Controlling the random generation of numbers by setting the seed number\nSoftware version ‚Äî The versions of your Python or R (or any other programming language) and its dependencies (e.g., libraries) impact the outcome of your code. For example, code that was built with pandas v1.0 may not run on v2.0 due to deprecation of functions\nOperating System (OS) ‚Äî Most programming languages, particularly R and Python, use different compilers (e.g., C, C++, etc.) and other built-in OS components. The type of OS and its version could impact the outcome of your code\nHardware ‚Äî Last but not, the type of hardware (or infrastructure) could impact your results (ARM/Intel/Apple processor, etc)\n\nIn a regular data science workflow, reproducibility is dependent on several factors. One of the most basic and trivial elements of reproducibility is code versioning. Using unversioned code makes it impossible to track changes in your code or verify that the same code runs on different environments.\nAnother critical factor is package versioning. Over time, packages and their dependencies tend to change and evolve, get new features, bug fixes, replace and deprecate old functions. In some cases, running your code using a specific package version may not work or yield the same results as older or newer versions of the package. For example, code written with Pandas 1.x may not work with Pandas 2.x, and vice versa.\nSimilarly, programming languages tend to change over time, and using code built with older versions may not work or be supported with recent ones. Moreover, different operating systems use different software architectures or run different types of compilers (c, c++, etc.) on the backend. This difference may impact the reproducibility of your code.\nLastly, the type of CPU architecture (ARM, Intel, Apple Silocon, etc.) may impact the underlying software. Some packages or software may require a separate build or may not be supported, which can ultimately affect the reproducibility of your code.\nBelow is Figure 1, which illustrates the factors affecting code reproducibility when transferring between different environments.\n\n\n\n\nFigure 1 - Shifting code between environments, what could go wrong?\n\n\n\nGit provides a solution for versioning and monitoring code, ensuring that it can be reproduced correctly regardless of the user or machine it runs on, as long as it is used properly. Docker and similar solutions address the problem of environment mismatches by creating an isolated environment within a container that can be shipped along with the code to any remote machine, such as a desktop, laptop or server, allowing for seamless reproduction of the process.\nWhile developing software on different hardware architectures, such as Apple Silicon and Intel-based machines, there may be potential differences in the environment. Docker can partially address this issue by creating a dedicated image for each CPU architecture. However, this approach can be time-consuming and expensive since additional tests are required to ensure that all containers have the exact same characteristics.\nFigure 2 demonstrates a general workflow with Docker and Git. We use Git and Github/Gitlab/Bitbucket (or any similar service) to code version control. Docker is used to set up a containerized environment, which will be used for code development and testing. We then shift our code with the container to any remote environment that supports containers (i.e., Github Actions, AWS, GCP, etc.).\n\n\n\n\nFigure 2 - Shifting code between environments with version control and containers\n\n\n\nWhile this workflow provides a high level of reproducibility, it does not cover reproducibility issues you may encounter due to different hardware settings. There are different methods to address this issue, such as building a dedicated environment for each hardware architecture.\nNote: Using a virtual environment is not an alternative to Docker. It actually works well together. While VE is not in the scope of this tutorial, you can read more about the differences between VE and Docker in the following article.",
    "crumbs": [
      "Home",
      "Workflow"
    ]
  },
  {
    "objectID": "cli-commands.html",
    "href": "cli-commands.html",
    "title": "CLI Commands",
    "section": "",
    "text": "This section focuses on the command line (CLI) core commands. Typically, when working with Docker and containers, we will use some flavor of Linux OS, such as Ubuntu, Alpine, etc. Throughout this workshop, we will use Ubuntu unless stated otherwise. We will start by pulling the official Ubuntu image and launch a container using the run command from the terminal:\nWe use the interactive and tty arguments to ssh inside the container:\nThis will ssh into the container terminal:",
    "crumbs": [
      "Home",
      "CLI Commands"
    ]
  },
  {
    "objectID": "cli-commands.html#basic-cli-commands",
    "href": "cli-commands.html#basic-cli-commands",
    "title": "CLI Commands",
    "section": "Basic CLI Commands",
    "text": "Basic CLI Commands\nThe ls command lists the files and directories in the current directory (or a different directory when provided a path):\nroot@0e52c467700c:/# ls\nbin  boot  dev  etc  home  lib  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\nBy default, the ls command won‚Äôt display hidden files (e.g., starts with .). To display all files, use the -a argument:\nroot@0e52c467700c:/# ls -a\n.  ..  .dockerenv  bin  boot  dev  etc  home  lib  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\nSimilarly, the -l argument will display the output in a long listing format:\nroot@0e52c467700c:/# ls -l\ntotal 48\nlrwxrwxrwx   1 root root    7 Jan 25 14:04 bin -&gt; usr/bin\ndrwxr-xr-x   2 root root 4096 Apr 18  2022 boot\ndrwxr-xr-x   5 root root  360 Feb  4 06:39 dev\ndrwxr-xr-x   1 root root 4096 Feb  4 06:39 etc\ndrwxr-xr-x   2 root root 4096 Apr 18  2022 home\nlrwxrwxrwx   1 root root    7 Jan 25 14:04 lib -&gt; usr/lib\ndrwxr-xr-x   2 root root 4096 Jan 25 14:04 media\ndrwxr-xr-x   2 root root 4096 Jan 25 14:04 mnt\ndrwxr-xr-x   2 root root 4096 Jan 25 14:04 opt\ndr-xr-xr-x 286 root root    0 Feb  4 06:39 proc\ndrwx------   2 root root 4096 Jan 25 14:09 root\ndrwxr-xr-x   5 root root 4096 Jan 25 14:09 run\nlrwxrwxrwx   1 root root    8 Jan 25 14:04 sbin -&gt; usr/sbin\ndrwxr-xr-x   2 root root 4096 Jan 25 14:04 srv\ndr-xr-xr-x  12 root root    0 Feb  4 06:39 sys\ndrwxrwxrwt   2 root root 4096 Jan 25 14:09 tmp\ndrwxr-xr-x  11 root root 4096 Jan 25 14:04 usr\ndrwxr-xr-x  11 root root 4096 Jan 25 14:09 var\nThe cd command enables to change the shell working directory:\nroot@0e52c467700c:/# cd bin\nroot@0e52c467700c:/bin#\nThe pwd returns the current working directory:\nroot@0e52c467700c:/bin# pwd\n/bin\nLast but not least, the .. reference the directory above the current path:\ncd ..\nroot@0e52c467700c:/#",
    "crumbs": [
      "Home",
      "CLI Commands"
    ]
  },
  {
    "objectID": "cli-commands.html#working-with-files-and-directories",
    "href": "cli-commands.html#working-with-files-and-directories",
    "title": "CLI Commands",
    "section": "Working with files and directories",
    "text": "Working with files and directories\nThe mkdir (i.e., make directory) command enables to create a directory:\nroot@0e52c467700c:/# mkdir test\nroot@0e52c467700c:/# cd test/\nroot@0e52c467700c:/test#\nThere are many ways to create and view files via the CLI. In the following example, we will create a bash script named hellow_world.sh that print Hello World!. We will use the touch command to create an empty bash script file:\nroot@0e52c467700c:/test# touch hello_world.sh\nYou can use the ls command to confirm that the file was created:\nroot@0e52c467700c:/test# ls\nhello_world.sh\nIn the CLI, the echo command is used to print text and can be used to write text into a file with the &gt; symbol. As we want the bash script to print Hello World! we will write to the file echo \"Hello World!\":\necho echo \"Hello World!\" &gt; hello_world.sh\nTo print the content of the file, we will use the cat command:\nroot@0e52c467700c:/test# cat hello_world.sh\necho Hello World!\nWe can now execute the file using the bash command:\nroot@0e52c467700c:/test# bash hello_world.sh\nHello World!\nNote: The &gt; symbol is used to add new text to a file. If the file already has any text, it will overwrite the previous text with the new text. To append new text, use the &gt;&gt; symbol:\nroot@0e52c467700c:/test# echo echo \"Hello World 2!\" &gt;&gt; hello_world.sh\nroot@0e52c467700c:/test# cat hello_world.sh\necho Hello World!\necho Hello World 2!\nroot@0e52c467700c:/test# bash hello_world.sh\nHello World!\nHello World 2!\nThe rm command (i.e., remove), enables to delete or remove files and folders. For example, let‚Äôs now delete the bash script we created above - hello_world.sh:\nroot@0e52c467700c:/test# rm hello_world.sh\nYou can use the ls command to confirm that the file was deleted:\nroot@0e52c467700c:/test# ls\nroot@0e52c467700c:/test#\nSimilarly, you can use the rm command to delete folders. Let‚Äôs remove the test folder we created:\nrm -rf test/\nThe r arguments are used to remove directories and their contents recursively, and the f argument to ignore nonexistent files and arguments.",
    "crumbs": [
      "Home",
      "CLI Commands"
    ]
  },
  {
    "objectID": "cli-commands.html#variables",
    "href": "cli-commands.html#variables",
    "title": "CLI Commands",
    "section": "Variables",
    "text": "Variables\nAnother common use case of command line functionality is the use of variables and environment variables. Assigning values to variables from the CLI is straightforward and fairly similar to other programming languages, such as R and Python, using the = symbol. For example, let‚Äôs assign the variable x the value 10:\nroot@0e52c467700c:/# x=10\nNote:Unlike R or Python, the CLI is sensitive to spaces when assigning values to variables. Using spaces would end up with error as it\nroot@0e52c467700c:/# x = 10\nbash: x: command not found\nSimilarly, typing the variable name would end up with an error, as the shell interpreter is expecting a command. For example, the below code would results in an error:\nroot@0e52c467700c:/# x\nbash: x: command not found\nTo refer to variables on the CLI, attach the $ sign before the variable name. For example, we will use the echo command to print the variable x we created above:\nroot@0e52c467700c:/# echo $x\n10\n\nAssign Variable to Bash Script\nWhen building containers, we often use bash scripts as helper files. For example, the following bash script is used to install Quarto during the build time of the image with the use of variables. It has the following functionality: - Receive input from the user with the Quarto version and assign the value to a variable name QUARTO_VERSION - Use the uname command to evaluate the CPU type and assign it to a variable name CPU - Use the wget function to pull the relevant Quarto build and install it with the dpkg function\ninstall_quarto.sh\n#!/usr/bin/env bash\nQUARTO_VERSION=$1 \n\necho \"Installing Quarto version $QUARTO_VERSION\"\n\n# Identify the CPU type (M1 vs Intel)\nif [[ $(uname -m) ==  \"aarch64\" ]] ; then\n  CPU=\"arm64\"\nelif [[ $(uname -m) ==  \"arm64\" ]] ; then\n  CPU=\"arm64\"\nelse\n  CPU=\"amd64\"\nfi\n\nTEMP_QUARTO=\"$(mktemp)\" && \\\n    wget -q  -O \"$TEMP_QUARTO\" https://github.com/quarto-dev/quarto-cli/releases/download/v$QUARTO_VERSION/quarto-${QUARTO_VERSION}-linux-${CPU}.deb && \\\n    dpkg -i \"$TEMP_QUARTO\" && \\\n    rm -f \"$TEMP_QUARTO\"\nHere is how you would execute the script:\nbash install_quarto.sh \"1.3.450\"\nWhere bash will index the variables by their order of assignment starting with 1. In the above case, as we have a single variable, it will be indexed as 1 and referenced in the script as $1\n\n\nSetting Environment Variables\nThe use of environment variables in the Docker development workflow is a very useful tool. It enables you to unify your settings with having multiple users. In addition, it enables you to parameterize your settings and avoid hard-coding some of the functionality of your image, such as the R or Python versions. We use the export command on the CLI to set an environment variable. For example, let‚Äôs reset x as an environment variable and assign the value 10 from the CLI:\nroot@0e52c467700c:/# export x=10\nroot@0e52c467700c:/# echo $x\n10\nOn Linux distro, you typically use the bash .bashrc file to add your setting and set environment variables. For example, if we want to set the x variabe we created before as environment variable we would add the",
    "crumbs": [
      "Home",
      "CLI Commands"
    ]
  },
  {
    "objectID": "01-settings.html#workshop-materials",
    "href": "01-settings.html#workshop-materials",
    "title": "Settings",
    "section": "Workshop Materials",
    "text": "Workshop Materials\nAll the workshop materials avaialble on the below repository:\nhttps://github.com/RamiKrispin/sdsu-docker-workshop\nPlease fork and clone to your local machine.",
    "crumbs": [
      "Home",
      "Settings"
    ]
  }
]